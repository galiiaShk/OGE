{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33de3270",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install opencv-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006eceeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1a8db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --user numpy==1.24.3 --upgrade #probably with some limitation because of --user space installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcba23f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import time\n",
    "\n",
    "from google.protobuf.json_format import MessageToDict \n",
    "\n",
    "from numpy import linalg\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7023494",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffc620d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part is only necessary to prepare your images (photos) for training\n",
    "# Currently all images will be read from folder \"data\" and subfolders of it will act as labels\n",
    "# please use numbers for subfolder names\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "hands = mp_hands.Hands(static_image_mode=True, min_detection_confidence=0.3)\n",
    "\n",
    "DATA_DIR = './data'\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "for dir_ in os.listdir(DATA_DIR):\n",
    "    for img_path in os.listdir(os.path.join(DATA_DIR, dir_)):\n",
    "        data_aux = []\n",
    "\n",
    "        x_ = []\n",
    "        y_ = []\n",
    "\n",
    "        img = cv2.imread(os.path.join(DATA_DIR, dir_, img_path))\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        results = hands.process(img_rgb)\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                for i in range(len(hand_landmarks.landmark)):\n",
    "                    x = hand_landmarks.landmark[i].x\n",
    "                    y = hand_landmarks.landmark[i].y\n",
    "\n",
    "                    x_.append(x)\n",
    "                    y_.append(y)\n",
    "\n",
    "                for i in range(len(hand_landmarks.landmark)):\n",
    "                    x = hand_landmarks.landmark[i].x\n",
    "                    y = hand_landmarks.landmark[i].y\n",
    "                    data_aux.append(x - min(x_))\n",
    "                    data_aux.append(y - min(y_))\n",
    "\n",
    "            data.append(data_aux)\n",
    "            labels.append(dir_)\n",
    "\n",
    "#Save data            \n",
    "f = open('data.pickle', 'wb')\n",
    "pickle.dump({'data': data, 'labels': labels}, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa4987e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part is only necessary to TRAIN RandomForestClassifier on your pickle data\n",
    "\n",
    "data_dict = pickle.load(open('./data.pickle', 'rb'))\n",
    "\n",
    "# You can print it out to check if the data structure is homogeneous\n",
    "### print(data_dict.keys())\n",
    "### print(data_dict)\n",
    "\n",
    "#Data and Labels are lists, so they have to be converted here\n",
    "Data_type = object\n",
    "\n",
    "data = np.asarray(data_dict['data'])\n",
    "labels = np.asarray(data_dict['labels'])\n",
    "\n",
    "#Dataset is split here in 80% training and 20% test data\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, shuffle=True, stratify=labels)\n",
    "\n",
    "#Simple but very fast modeltype to train - good for older hardware/training on cpu\n",
    "model = RandomForestClassifier()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "y_predict = model.predict(x_test)\n",
    "score = accuracy_score(y_predict, y_test)\n",
    "\n",
    "print('{}% of samples are classified correctly '.format(score * 100))\n",
    "\n",
    "#save model\n",
    "f = open('model.p', 'wb')\n",
    "pickle.dump({'model': model}, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31f9c8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "247ca378",
   "metadata": {},
   "outputs": [],
   "source": [
    "def are_finger_stretched(point1, point2, point3):\n",
    "    outcome = 0\n",
    "         #\n",
    "    distance1 = np.linalg.norm((point2 - point1), ord=2)\n",
    "    distance2 = np.linalg.norm((point3 - point1), ord=2)\n",
    "    if distance2 > distance1:\n",
    "        outcome = 1\n",
    "    \n",
    "    #check the result\n",
    "    #print(outcome, '\\n')\n",
    "    return outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25884c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_gesture(result):\n",
    "    if (result[0] == 1) and (result[1] == 0) and (result[2] == 0) and (result[3] == 0) and (result[4] == 0):\n",
    "        gesture = \"good\"\n",
    "    elif (result[0] == 1) and (result[1] == 1)and (result[2] == 0) and (result[3] == 0) and (result[4] == 0):\n",
    "        gesture = \"full barre\"\n",
    "    elif (result[0] == 0) and (result[1] == 1)and (result[2] == 0) and (result[3] == 0) and (result[4] == 0):\n",
    "        gesture = \"one\"\n",
    "    elif (result[0] == 0) and (result[1] == 0)and (result[2] == 1) and (result[3] == 0) and (result[4] == 0):\n",
    "        gesture = \"VERY BAD\"\n",
    "    elif (result[0] == 0) and (result[1] == 1)and (result[2] == 1) and (result[3] == 0) and (result[4] == 0):\n",
    "        gesture = \"two\"\n",
    "    elif (result[0] == 1) and (result[1] == 1)and (result[2] == 1) and (result[3] == 0) and (result[4] == 0):\n",
    "        gesture = \"3 finger picking\"\n",
    "    elif (result[0] == 0) and (result[1] == 1)and (result[2] == 1) and (result[3] == 1) and (result[4] == 0):\n",
    "        gesture = \"three\"\n",
    "    elif (result[0] == 0) and (result[1] == 1)and (result[2] == 1) and (result[3] == 1) and (result[4] == 1):\n",
    "        gesture = \"four\"\n",
    "    elif (result[0] == 1) and (result[1] == 1)and (result[2] == 1) and (result[3] == 1) and (result[4] == 1):\n",
    "        gesture = \"five\"\n",
    "    elif (result[0] == 1) and (result[1] == 0)and (result[2] == 0) and (result[3] == 0) and (result[4] == 1):\n",
    "        gesture = \"Shaka\"\n",
    "    elif (result[0] == 0) and (result[1] == 0)and (result[2] == 1) and (result[3] == 1) and (result[4] == 1):\n",
    "        gesture = \"OK or plectrum\"\n",
    "    elif(result[0] == 0) and (result[1] == 0) and (result[2] == 0) and (result[3] == 0) and (result[4] == 0):\n",
    "        gesture = \"closed\"\n",
    "    else:\n",
    "        gesture = \"unknown/not detected\"\n",
    "    \n",
    "    return gesture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4554cf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86a8cc08",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "model_dict = pickle.load(open('./model.p', 'rb'))\n",
    "model = model_dict['model']\n",
    "\n",
    "labels_dict = {0: 'Minor1', 1: 'Minor2', 2: 'MajorToMoll1', 3: 'MajorToMoll2', 4: 'FullBarre1'}\n",
    "\n",
    "width = 1280\n",
    "height = 720\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
	"# Delete following settings and replace resize function for more FPS\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH,width)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT,height)\n",
    "cap.set(cv2.CAP_PROP_FPS, 30)\n",
    "\n",
    "hands = mp.solutions.hands.Hands(static_image_mode=False,\n",
    "                         max_num_hands=2,\n",
    "                         min_tracking_confidence=0.5,\n",
    "                         min_detection_confidence=0.5)\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "previousTime = 0\n",
    "currentTime = 0\n",
    "\n",
    "# Array for 5 fingers as figure\n",
    "figure = [0 for element in range(5)]\n",
    "\n",
    "# Array for 0-20 trackpoints and 2 hands\n",
    "landmark = np.empty((21, 2))\n",
    "\n",
    "while True:\n",
    "    _, frame = cap.read()\n",
    "    \n",
    "    #Horizontally flip the image to get right hand for the right and left for the left hand\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    frame = cv2.resize(frame,(width,height))\n",
    "    \n",
    "    #Convert image from webcam back to RGB\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    #Should improve performance, but no difference to see\n",
    "    #frame_rgb.flags.writeable = False\n",
    "    \n",
    "    results = hands.process(frame_rgb)\n",
    "    \n",
    "    if results.multi_hand_landmarks:\n",
    "        #frame_rgb.flags.writeable = True\n",
    "        for handsnumber, lms in enumerate(results.multi_hand_landmarks):\n",
    "            mp_drawing.draw_landmarks(frame, lms, mp_hands.HAND_CONNECTIONS)\n",
    "            \n",
    "            for i in results.multi_handedness:\n",
    "                \n",
    "                # writes the information like index, score and label of hand\n",
    "                label = MessageToDict(i)[\n",
    "                    'classification'][0]['label']\n",
    "                \n",
    "                \n",
    "                if label == 'Left':\n",
    "                    \n",
    "                    # If left hand is recognized it will  \n",
    "                    data_aux = []\n",
    "                    x_ = []\n",
    "                    y_ = []\n",
    "\n",
    "                    H, W, _ = frame.shape\n",
    "\n",
    "                    for hand_landmarks in results.multi_hand_landmarks:\n",
    "                    \n",
    "                    # alternative CODE in COMMENT is not finished yet\n",
    "                    #for idx, classification in enumerate(results.multi_handedness):\n",
    "                    #    if classification.classification[0].index == 0:\n",
    "                            \n",
    "                            mp_drawing.draw_landmarks(\n",
    "                                frame,  # image to draw\n",
    "                                hand_landmarks,  # model output\n",
    "                                mp_hands.HAND_CONNECTIONS,  # hand connections\n",
    "                                mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                                mp_drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "                            for i in range(len(hand_landmarks.landmark)):\n",
    "                                x = hand_landmarks.landmark[i].x\n",
    "                                y = hand_landmarks.landmark[i].y\n",
    "                                x_.append(x)\n",
    "                                y_.append(y)\n",
    "\n",
    "                            for i in range(len(hand_landmarks.landmark)):\n",
    "                                x = hand_landmarks.landmark[i].x\n",
    "                                y = hand_landmarks.landmark[i].y\n",
    "                                data_aux.append(x - min(x_))\n",
    "                                data_aux.append(y - min(y_))\n",
    "\n",
    "                            x1 = int(min(x_) * W) - 10\n",
    "                            y1 = int(min(y_) * H) - 10\n",
    "\n",
    "                            x2 = int(max(x_) * W) - 10\n",
    "                            y2 = int(max(y_) * H) - 10\n",
    "\n",
    "                            if (len(data_aux) == 42):\n",
    "                                prediction = model.predict([np.asarray(data_aux)])\n",
    "                                predicted_character = labels_dict[int(prediction[0])]\n",
    "\n",
    "                                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 0), 4)\n",
    "                                cv2.putText(frame, predicted_character, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 1.3, (0, 0, 0), 3,\n",
    "                                                cv2.LINE_AA)\n",
    "                            \n",
    "                 \n",
    "                    \n",
    "                if label == 'Right':\n",
    "            \n",
    "                    #for id, lm in enumerate(lms.landmark):\n",
    "                    if len(results.multi_hand_landmarks) > 1:\n",
    "                        for id, lm in enumerate(results.multi_hand_landmarks[1].landmark): #ONLY FOR ONE HAND WITHOUT SECOND for LOOP\n",
    "\n",
    "                            h, w, _ = frame.shape\n",
    "                            cx = int(lm.x * w) \n",
    "                            cy = int(lm.y * h)\n",
    "                            cv2.circle(frame, (cx, cy), 3, (128, 128, 128))\n",
    "\n",
    "                            landmark_ = [cx, cy]\n",
    "                            landmark[id,:] = landmark_\n",
    "                            #print(id)\n",
    "                            #print(lm)\n",
    "\n",
    "                        #Marks the top of fingers with different colors and thickness\n",
    "                            if id == 4:\n",
    "                                cv2.circle(frame, (cx, cy), 6, (0, 0, 0), cv2.FILLED)\n",
    "                            if id == 8:\n",
    "                                cv2.circle(frame, (cx, cy), 12, (255, 0, 0), cv2.FILLED)\n",
    "                            if id == 12:\n",
    "                                cv2.circle(frame, (cx, cy), 12, (0, 255, 0), cv2.FILLED)\n",
    "                            if id == 16:\n",
    "                                cv2.circle(frame, (cx, cy), 12, (0, 0, 255), cv2.FILLED)\n",
    "                            if id == 20:\n",
    "                                cv2.circle(frame, (cx, cy), 12, (255, 0, 255), cv2.FILLED)\n",
    "\n",
    "                        for i in range(5):\n",
    "                            if i == 0:\n",
    "                                #pinky\n",
    "                                figure_ = are_finger_stretched(landmark[17],landmark[4*i+2],landmark[4*i+4])\n",
    "                            else:\n",
    "                                #wrest\n",
    "                                figure_ = are_finger_stretched(landmark[0],landmark[4*i+2],landmark[4*i+4])\n",
    "\n",
    "                            figure[i] = figure_\n",
    "                        #print(figure, '\\n')\n",
    "\n",
    "                        gesture_solution = detect_gesture(figure)\n",
    "                        cv2.putText(frame, f\"{gesture_solution}\", (240, 35*(handsnumber+1)), cv2.FONT_HERSHEY_PLAIN, 2, (255, 255, 0), 5)\n",
    "\n",
    "            \n",
    "    #Shows Frames per Second\n",
    "        currentTime = time.time()\n",
    "        fps = 1/(currentTime-previousTime)\n",
    "        previousTime = currentTime        \n",
    "        cv2.putText(frame, str(int(fps)), (10, 35), cv2.FONT_HERSHEY_PLAIN, 3, (255, 0, 255), 3) #Pos,Font,Scale,Color,Thick\n",
    "            \n",
    "        cv2.imshow(\"Hand Tracking\", frame) #Name of Window and the Live-Image\n",
    "        \n",
    "    if cv2.waitKey(1) == ord('q') : \n",
    "        #quit with Q\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b113ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ef2e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative Method to save information about left and right hand\n",
    "# This should be studied in case, if 1 and 2 approach does NOT WORK\n",
    "\n",
    "\"\"\"\n",
    "import cv2\n",
    "\n",
    "class mpHands:\n",
    "    import mediapipe as mp\n",
    "    \n",
    "    def __init__(self,maxHands=2,tol1=.5,tol2=.5):\n",
    "        self.hands=self.mp.solutions.hands.Hands(False,maxHands,tol1,tol2)\n",
    "        \n",
    "    def Marks(self,frame):\n",
    "        myHands=[]\n",
    "        handsType=[]\n",
    "        frameRGB=cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n",
    "        results=self.hands.process(frameRGB)\n",
    "        if results.multi_hand_landmarks != None:\n",
    "            #print(results.multi_handedness)\n",
    "            for hand in results.multi_handedness:\n",
    "                #print(hand)\n",
    "                #print(hand.classification)\n",
    "                #print(hand.classification[0])\n",
    "                handType=hand.classification[0].label\n",
    "                handsType.append(handType)\n",
    "            for handLandMarks in results.multi_hand_landmarks:\n",
    "                myHand=[]\n",
    "                for landMark in handLandMarks.landmark:\n",
    "                    myHand.append((int(landMark.x*width),int(landMark.y*height)))\n",
    "                myHands.append(myHand)\n",
    "        return myHands,handsType\n",
    " \n",
    "#width=1280\n",
    "#height=720\n",
    "#cam=cv2.VideoCapture(0,cv2.CAP_DSHOW)\n",
    "#cam.set(cv2.CAP_PROP_FRAME_WIDTH, width)\n",
    "#cam.set(cv2.CAP_PROP_FRAME_HEIGHT,height)\n",
    "#cam.set(cv2.CAP_PROP_FPS, 30)\n",
    "#cam.set(cv2.CAP_PROP_FOURCC,cv2.VideoWriter_fourcc(*'MJPG'))\n",
    "\n",
    "cam = cv2.VideoCapture(0)\n",
    "findHands=mpHands(2)\n",
    "\n",
    "while True:\n",
    "    ignore,  frame = cam.read()\n",
    "    frame=cv2.flip(frame, 1)\n",
    "    handData, handsType=findHands.Marks(frame)\n",
    "    for hand,handType in zip(handData,handsType):\n",
    "        if handType=='Right':\n",
    "            handColor=(255,0,0)\n",
    "        if handType=='Left':\n",
    "            handColor=(0,0,255)\n",
    "        for ind in [0,5,6,7,8]:\n",
    "            cv2.circle(frame,hand[ind],15,handColor,5)\n",
    "    cv2.imshow('Name of window', frame)\n",
    "    cv2.moveWindow('Name of window',0,0)\n",
    "    if cv2.waitKey(1) & 0xff ==ord('q'):\n",
    "        break\n",
    "cam.release()\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54656cd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a76c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Backup for RandomForest Approach trained for 42 Parameters (= 1 Hand)\n",
    "'''\n",
    "model_dict = pickle.load(open('./model.p', 'rb'))\n",
    "model = model_dict['model']\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "hands = mp_hands.Hands(static_image_mode=True, min_detection_confidence=0.3)\n",
    "\n",
    "labels_dict = {0: 'Minor1', 1: 'Minor2', 2: 'MajorToMoll1', 3: 'MajorToMoll2', 4: 'FullBarre1'}\n",
    "\n",
    "\n",
    "while True:\n",
    "\n",
    "    data_aux = []\n",
    "    x_ = []\n",
    "    y_ = []\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    H, W, _ = frame.shape\n",
    "\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    results = hands.process(frame_rgb)\n",
    "    if results.multi_hand_landmarks:\n",
    "        \n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame,  # image to draw\n",
    "                hand_landmarks,  # model output\n",
    "                mp_hands.HAND_CONNECTIONS,  # hand connections\n",
    "                mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                mp_drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            for i in range(len(hand_landmarks.landmark)):\n",
    "                \n",
    "                x = hand_landmarks.landmark[i].x\n",
    "                y = hand_landmarks.landmark[i].y\n",
    "                x_.append(x)\n",
    "                y_.append(y)\n",
    "\n",
    "            for i in range(len(hand_landmarks.landmark)):\n",
    "                \n",
    "                x = hand_landmarks.landmark[i].x\n",
    "                y = hand_landmarks.landmark[i].y\n",
    "                data_aux.append(x - min(x_))\n",
    "                data_aux.append(y - min(y_))\n",
    "\n",
    "        x1 = int(min(x_) * W) - 10\n",
    "        y1 = int(min(y_) * H) - 10\n",
    "\n",
    "        x2 = int(max(x_) * W) - 10\n",
    "        y2 = int(max(y_) * H) - 10\n",
    "\n",
    "        prediction = model.predict([np.asarray(data_aux)])\n",
    "        predicted_character = labels_dict[int(prediction[0])]\n",
    "\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 0), 4)\n",
    "        cv2.putText(frame, predicted_character, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 1.3, (0, 0, 0), 3,\n",
    "                    cv2.LINE_AA)\n",
    "                            \n",
    "\n",
    "    cv2.imshow('frame', frame)\n",
    "    cv2.waitKey(1)\n",
    "\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
