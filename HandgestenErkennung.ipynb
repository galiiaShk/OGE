{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import keyboard \n",
    "import numpy as np\n",
    "import json\n",
    "from KeyBindings import quit, readCoords,writeCoords\n",
    "#init\n",
    "#---------------------\n",
    "cap= cv2.VideoCapture(0)\n",
    "\n",
    "hands= mp.solutions.hands.Hands(static_image_mode=False,\n",
    "                        max_num_hands=1,\n",
    "                        min_tracking_confidence=0.5,\n",
    "                        min_detection_confidence=0.5)\n",
    "\n",
    "mpDraw = mp.solutions.drawing_utils\n",
    "\n",
    "#---------------------------------------     \n",
    "def ScaleAndRotateHand():\n",
    "    \n",
    "    pivotTrackedWritst = int( result.multi_hand_landmarks[0].landmark[0].x*w), int( result.multi_hand_landmarks[0].landmark[0].y*h)\n",
    "    pivotTrackedFirstBone = int( result.multi_hand_landmarks[0].landmark[1].x*w), int( result.multi_hand_landmarks[0].landmark[1].y*h)\n",
    "    #print(np.dot(pivotTracked[1]-pivotTracked[0],pivotOffsetImageRatioXY[1]-pivotOffsetImageRatioXY[0]))\n",
    "    print(pivotTrackedFirstBone- pivotTrackedWritst)\n",
    "#---------------------------------------   \n",
    "# draw the standard hand from mediapipe\n",
    "def drawHandMediaPipe(img, lm):\n",
    "    h, w, _ = img.shape\n",
    "    cx, cy = int(lm.x*w), int(lm.y*h)\n",
    "    cv2.circle(img, (cx, cy), 3, (255, 0, 255)) \n",
    "    mpDraw.draw_landmarks(img, result.multi_hand_landmarks[0], mp.solutions.hands.HAND_CONNECTIONS)\n",
    "#---------------------------------------   \n",
    "#Draws the saved gesture, the one from media pipe and compares both,\n",
    "# TODO: THis function should be split in two for a single recognition part\n",
    "def drawSavedHandGesture(id,h,w, recognizedColor):\n",
    "    \n",
    "     if len(savedCoords[0])<=2:\n",
    "            return\n",
    "     else:\n",
    "        #Set the percentual offset for the wrist when the hand is scaled\n",
    "        reciproce =rootBoone[2]/ savedCoords[0][2]\n",
    "     # get the hand wrist as compare offset x,y,z and merge them to the xyz\n",
    "        pivotPointX = (result.multi_hand_landmarks[0].landmark[1].x-(savedCoords[0][0]*reciproce))\n",
    "        pivotPointY = (result.multi_hand_landmarks[0].landmark[1].y -(savedCoords[0][1]*reciproce))\n",
    "        pivotPointZ = (result.multi_hand_landmarks[0].landmark[1].z -savedCoords[0][2])\n",
    "        pivotXYZ = pivotPointX,pivotPointY,pivotPointZ\n",
    "        #Draw the tracked handgesture as before but without the lines in between\n",
    "        pivotTracked = int( result.multi_hand_landmarks[0].landmark[id].x*w), int( result.multi_hand_landmarks[0].landmark[id].y*h)\n",
    "        # Calculate the savedCoordinates with the offset of the current projected gesture to match the hand wrist position independed of the screen\n",
    "        savedOffset = np.dot(savedCoords[id],rootBoone[2]/savedCoords[0][2] )\n",
    "        \n",
    "        savedOffset =  [savedOffset[0]+pivotXYZ[0],  savedOffset[1]+pivotXYZ[1],  savedOffset[2]+pivotXYZ[2]]\n",
    "       \n",
    "        # For visual representation, it has to be redrawn in respect to the width and height ratio of the screen\n",
    "        pivotOffsetImageRatio = [int (savedOffset[0]*w), int (savedOffset[1]*h), savedOffset[2]]\n",
    "        pivotOffsetImageRatioXY = int ((savedOffset[0])*w), int ((savedOffset[1])*h)\n",
    "        scaleBone =  int (rootBoone[0]-(pivotOffsetImageRatio[2]*15)*(pivotOffsetImageRatioXY[0]-rootBoone[0])), int (rootBoone[1]-(pivotOffsetImageRatio[2]*15)*(pivotOffsetImageRatioXY[1]-rootBoone[1]))\n",
    "        diffXy=abs((np.subtract(pivotTracked,pivotOffsetImageRatioXY)))\n",
    "        \n",
    "        pivotTrackedWritst = int( result.multi_hand_landmarks[0].landmark[0].x*w), int( result.multi_hand_landmarks[0].landmark[0].y*h)\n",
    "        pivotTrackedFirstBone = int( result.multi_hand_landmarks[0].landmark[1].x*w), int( result.multi_hand_landmarks[0].landmark[1].y*h)\n",
    "        #print(np.dot(pivotTracked[1]-pivotTracked[0],pivotOffsetImageRatioXY[1]-pivotOffsetImageRatioXY[0]))\n",
    "       \n",
    "        #Compare gestures\n",
    "        if(diffXy[0]<50 and diffXy[1]<50):\n",
    "            recognizedColor =(0,255,0)\n",
    "        #draw the circle for the tracked hand\n",
    "        #print(\"draw saved gestures\",id,pivotOffsetImageRatio[2]*30)\n",
    "        cv2.circle(img, (pivotTracked), 5, (255, 0, 0)) \n",
    "        #draw the circle for the saved matching hand gesture\n",
    "        cv2.circle(img ,pivotOffsetImageRatioXY,3, recognizedColor)\n",
    "     \n",
    "        return pivotOffsetImageRatio\n",
    "#Init params\n",
    "savedGesturesFilePath=\"handGestures.txt\"\n",
    "savedImageFilePath=\"Image.txt\"\n",
    "recognizedPoints = np.array([0,0,0])\n",
    "savedOffsetPoints= np.array([0,0,0])\n",
    "#savedCoords = readCoords(True, savedImageFilePath)\n",
    "savedCoords = readCoords(True, savedGesturesFilePath)\n",
    "recognizedColor = (0,0,255)\n",
    "rootBoone = []\n",
    "\n",
    "while True:    \n",
    "    _,img = cap.read()\n",
    "    result= hands.process(img)\n",
    "   # if the gestrues should be compared this frame, start the compare method in the for loop\n",
    "    recognizedPoints = []# clear the array before the loop starts to avoid overflow\n",
    "    if result.multi_hand_landmarks:\n",
    "        \n",
    "        h, w, _ = img.shape\n",
    "        savedOffsetPoints = []# clear the array before the loop starts to avoid overflow  \n",
    "        rootBoone = result.multi_hand_landmarks[0].landmark[0]\n",
    "        rootBoone = rootBoone.x*w, rootBoone.y*h, rootBoone.z\n",
    "       \n",
    "        \n",
    "        # for each recognized point on the hand, get the id and the bone coordinates\n",
    "        for id, lm in enumerate(result.multi_hand_landmarks[0].landmark):   \n",
    "            #calculate the saved handgesture in respect to the current hand position to make it independed of the image aspect\n",
    "            savedOffsetPoints.append(drawSavedHandGesture(id,h,w, recognizedColor))\n",
    "            #set the default points of the current handpostion\n",
    "            recognizedPoints.append([lm.x, lm.y, lm.z])# append the circles from the hand to the array, 21 in total        \n",
    "       \n",
    "        # default hand drawing from puling\n",
    "        #drawHandMediaPipe(img, lm)\n",
    "        # Draw the result\n",
    "    cv2.imshow(\"Hand tracking test\", img)\n",
    "\n",
    "    if quit():\n",
    "        break\n",
    "    #writeCoords(img)\n",
    "    writeCoords(recognizedPoints, savedGesturesFilePath)\n",
    "    readCoords(False, savedGesturesFilePath)  \n",
    "  \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: \n",
    "*   Slider f체r Threshold etabielren, um genauigkeit der Gesten zu debuggen\n",
    "*   Alex Ansatz einbauen und schauen, ob das erfassen der Fingerspitzen schon ausreicht nach dem Motto: \n",
    "    Nur die Handspitzen sind f체r anf채nger, die korrekte Fingerhalteung ist dann das n채chste Schwierig, its level\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
